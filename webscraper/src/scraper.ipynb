{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preamble (required)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from IPython.display import display, Image\n",
    "import tempfile\n",
    "import os\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "from pymongo import MongoClient\n",
    "\n",
    "load_dotenv(\"../.env\")\n",
    "\n",
    "DUMP_DIR = \"./out\"\n",
    "OUTPUT_DIR = \"./txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create scraper with sanity test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TODO: scrape every event shop.\n",
    "###       only get the ones with a vaible shop (debug search for keyword item shop to check for compatability)\n",
    "###       Put bad entries in a list txt file and skip in the future.\n",
    "###       Serverless function? Python (reality: just rewrite in js)\n",
    "# https://azurlane.koumakan.jp/wiki/Events#Limited_Event_List\n",
    "\n",
    "\n",
    "def al_scraper(url, show=True):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "    en_start_date = None\n",
    "    en_end_date = None\n",
    "    all_start_date = None\n",
    "    all_end_date = None\n",
    "\n",
    "    # Find the EN Servers row and extract dates\n",
    "    event_table = soup.find(\"table\", class_=\"event-infobox\")\n",
    "    if event_table:\n",
    "        rows = event_table.find_all(\"tr\")\n",
    "        for i, row in enumerate(rows):\n",
    "            th = row.find(\"th\")\n",
    "            if th and \"en server\" in th.text.lower():\n",
    "                if i + 2 < len(rows):\n",
    "                    en_start_date = rows[i + 1].find_all(\"td\")[1].text.strip()\n",
    "                    en_end_date = rows[i + 2].find_all(\"td\")[1].text.strip()\n",
    "                    if show:\n",
    "                        print(\"EN Servers Date\")\n",
    "                        print(f\"Event period: {en_start_date} - {en_end_date}\")\n",
    "            elif th and \"all server\" in th.text.lower():\n",
    "                if i + 2 < len(rows):\n",
    "                    all_start_date = rows[i + 1].find_all(\"td\")[1].text.strip()\n",
    "                    all_end_date = rows[i + 2].find_all(\"td\")[1].text.strip()\n",
    "                    if show:\n",
    "                        print(\"All Servers Date\")\n",
    "                        print(f\"Event period: {all_start_date} - {all_end_date}\")\n",
    "\n",
    "    # Find all shop items\n",
    "    shop_items = soup.find_all(\"div\", class_=\"item-frame\")\n",
    "\n",
    "    # Extract and print details for each item\n",
    "    # Get total number of shop items\n",
    "    total_items = len(shop_items)\n",
    "\n",
    "    if not shop_items or total_items <= 0:\n",
    "        return False\n",
    "\n",
    "    if show:\n",
    "        print(f\"Total item (entries) in shop: {total_items}\")\n",
    "    for item in shop_items:\n",
    "        stock = (\n",
    "            item.find(\"div\", class_=\"item-stock\")\n",
    "            .text.strip()\n",
    "            .replace(\"Available:\", \"\")\n",
    "            .strip()\n",
    "        )\n",
    "        name = item.find(\"div\", class_=\"item-name\").text.strip()\n",
    "        price = item.find(\"div\", class_=\"item-price\").text.strip()\n",
    "        if show:\n",
    "            print(f\"Item: '{name}'\")\n",
    "            print(f\"Stock: '{stock}'\")\n",
    "            print(f\"Price: '{price}'\")\n",
    "        image_url = item.find(\"div\", class_=\"item-image-frame\").find(\"img\")[\"src\"]\n",
    "        if not image_url.startswith(\"http\"):\n",
    "            image_url = \"https://azurlane.koumakan.jp\" + image_url\n",
    "        # print(f\"Image URL: {image_url}\")\n",
    "\n",
    "        if show:\n",
    "            img = download_image(image_url, name)\n",
    "            display(Image(img))\n",
    "            print(\"--\")\n",
    "\n",
    "        # Create dictionary to store event info\n",
    "        event_data = {\"dates\": {}, \"shop_items\": []}\n",
    "\n",
    "        # Store dates\n",
    "        if en_start_date:\n",
    "            event_data[\"dates\"][\"en_server\"] = {\n",
    "                \"start\": en_start_date,\n",
    "                \"end\": en_end_date,\n",
    "            }\n",
    "        if all_start_date:\n",
    "            event_data[\"dates\"][\"all_server\"] = {\n",
    "                \"start\": all_start_date,\n",
    "                \"end\": all_end_date,\n",
    "            }\n",
    "\n",
    "        # Store shop items\n",
    "        for item in shop_items:\n",
    "            item_data = {\n",
    "                \"name\": item.find(\"div\", class_=\"item-name\").text.strip(),\n",
    "                \"stock\": item.find(\"div\", class_=\"item-stock\")\n",
    "                .text.strip()\n",
    "                .replace(\"Available:\", \"\")\n",
    "                .strip(),\n",
    "                \"price\": item.find(\"div\", class_=\"item-price\").text.strip(),\n",
    "                \"image_url\": item.find(\"div\", class_=\"item-image-frame\").find(\"img\")[\n",
    "                    \"src\"\n",
    "                ],\n",
    "            }\n",
    "            if not item_data[\"image_url\"].startswith(\"http\"):\n",
    "                item_data[\"image_url\"] = (\n",
    "                    \"https://azurlane.koumakan.jp\" + item_data[\"image_url\"]\n",
    "                )\n",
    "            event_data[\"shop_items\"].append(item_data)\n",
    "\n",
    "        return event_data\n",
    "\n",
    "\n",
    "def download_image(url, name):\n",
    "    temp_dir = tempfile.gettempdir()\n",
    "    temp_path = os.path.join(temp_dir, f\"{name}.jpg\")\n",
    "    with open(temp_path, \"wb\") as f:\n",
    "        f.write(requests.get(url).content)\n",
    "    return temp_path\n",
    "\n",
    "\n",
    "def picsum_demo():\n",
    "    temp_dir = tempfile.gettempdir()\n",
    "    # Download a random image from Lorem Picsum as comparison\n",
    "    lorem_url = \"https://picsum.photos/200/300\"\n",
    "    lorem_path = os.path.join(temp_dir, \"lorem_comparison.jpg\")\n",
    "    with open(lorem_path, \"wb\") as f:\n",
    "        f.write(requests.get(lorem_url).content)\n",
    "    print(f\"Lorem Picsum comparison saved to: {lorem_path}\")\n",
    "    display(Image(lorem_path))\n",
    "\n",
    "\n",
    "res = al_scraper(url=\"https://azurlane.koumakan.jp/wiki/Dangerous_Inventons_Incoming!\")\n",
    "# if res:\n",
    "#     print(res)\n",
    "# al_scraper(url = 'https://azurlane.koumakan.jp/wiki/Violet_Tempest,_Blooming_Lycoris')\n",
    "# al_scraper(url = 'https://azurlane.koumakan.jp/wiki/Visitors_Dyed_in_Red_Rerun')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Look for new URLs and parse existsing ones (concurrent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all event URLs from the events page\n",
    "event_list_url = \"https://azurlane.koumakan.jp/wiki/Events#Limited_Event_List\"\n",
    "response = requests.get(event_list_url)\n",
    "soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "# Find the Limited Event List section and get all table headers with links\n",
    "event_table = (\n",
    "    soup.find(\"span\", {\"id\": \"Limited_Event_List\"}).find_parent(\"h2\").find_next(\"table\")\n",
    ")\n",
    "event_links = event_table.find_all(\"th\")\n",
    "\n",
    "# Open a file to store events without shops\n",
    "# Create output directory if it doesn't exist\n",
    "if not os.path.exists(OUTPUT_DIR):\n",
    "    os.makedirs(OUTPUT_DIR)\n",
    "\n",
    "shopless_events = open(f\"{OUTPUT_DIR}/shopless_event_urls.txt\", \"a+\")\n",
    "shop_events = open(f\"{OUTPUT_DIR}/shop_event_urls.txt\", \"a+\")\n",
    "\n",
    "# Read existing URLs and remove duplicates\n",
    "existing_urls = set(shopless_events.readlines())\n",
    "shopless_events.seek(0)\n",
    "shopless_events.truncate()\n",
    "shopless_events.writelines(sorted(set(existing_urls)))\n",
    "\n",
    "\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "\n",
    "def process_event(link):\n",
    "    if link and link.get(\"href\"):\n",
    "        event_url = \"https://azurlane.koumakan.jp\" + link[\"href\"]\n",
    "        if event_url in existing_urls:\n",
    "            return\n",
    "        print(f\"Event URL: {event_url}\")\n",
    "        ret = al_scraper(event_url, show=False)\n",
    "        if not ret:\n",
    "            # print(\"\\tNo items found in shop.\")\n",
    "            shopless_events.write(event_url + \"\\n\")\n",
    "        else:\n",
    "            shop_events.write(event_url + \"\\n\")\n",
    "\n",
    "\n",
    "with ThreadPoolExecutor(max_workers=5) as executor:\n",
    "    for header in event_links:\n",
    "        a_links = header.find_all(\"a\")\n",
    "        executor.map(process_event, a_links)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process Good URLs into JSON dump"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 90 URLs\n",
      "Processing https://azurlane.koumakan.jp/wiki/Divergent_Chessboard\n",
      "Processing https://azurlane.koumakan.jp/wiki/Visitors_Dyed_in_Red_Rerun\n",
      "Processing https://azurlane.koumakan.jp/wiki/Opposite-Colored_Rerun\n",
      "Processing https://azurlane.koumakan.jp/wiki/Ink-Stained_Steel_Sakura\n",
      "Processing https://azurlane.koumakan.jp/wiki/Iris_of_Light_and_Dark\n",
      "Processing https://azurlane.koumakan.jp/wiki/Crimson_Echoes\n",
      "Processing https://azurlane.koumakan.jp/wiki/Hibiscus-scented_Idol\n",
      "Processing https://azurlane.koumakan.jp/wiki/Ink-Stained_Steel_Sakura_Rerun\n",
      "Processing https://azurlane.koumakan.jp/wiki/Scherzo_of_Iron_and_Blood\n",
      "Processing https://azurlane.koumakan.jp/wiki/Scherzo_of_Iron_and_Blood#Science_Rules.21\n",
      "done!\n"
     ]
    }
   ],
   "source": [
    "# Read shop event URLs and scrape each one\n",
    "with open(f\"{OUTPUT_DIR}/shop_event_urls.txt\", \"r\") as f:\n",
    "    shop_urls = f.readlines()\n",
    "\n",
    "if not shop_urls:\n",
    "    raise ValueError(\"No shop URLs found in shop_event_urls.txt\")\n",
    "\n",
    "if not os.path.exists(DUMP_DIR):\n",
    "    os.makedirs(DUMP_DIR)\n",
    "\n",
    "db_tmp = open(f\"{DUMP_DIR}/soon-to-be-db.json\", \"w\")\n",
    "\n",
    "# Clean URLs by removing whitespace/newlines\n",
    "shop_urls_clean = [url.strip() for url in shop_urls]\n",
    "\n",
    "# # use exceptions to debug\n",
    "# for url, clean_url in zip(shop_urls, shop_urls_clean):\n",
    "#     if url != clean_url:\n",
    "#         raise ValueError(f\"URL mismatch after cleaning:\\nOriginal: {url!r}\\nCleaned: {clean_url!r}\")\n",
    "# exit()\n",
    "\n",
    "# Process each URL\n",
    "db_j = {}\n",
    "\n",
    "\n",
    "# def process_shop_url(url):\n",
    "#     print(f\"Processing {url}\")\n",
    "#     res = al_scraper(url=url, show=False)\n",
    "#     return url, res\n",
    "\n",
    "\n",
    "# with ThreadPoolExecutor(max_workers=5) as executor:\n",
    "#     # Submit all tasks and get futures\n",
    "#     futures = [executor.submit(process_shop_url, url) for url in shop_urls_clean[:10]]\n",
    "\n",
    "#     # Process results as they complete\n",
    "#     for future in futures:\n",
    "#         url, res = future.result()\n",
    "#         print(f\"Done: {url}\")\n",
    "#         db_j[url] = res\n",
    "\n",
    "# Process URLs sequentially\n",
    "num_items = 10  # Set to None for all items, or a number for limited processing\n",
    "ni_string = \"All\"\n",
    "if num_items:\n",
    "    ni_string = num_items\n",
    "print(f\"Processing {ni_string} of {len(shop_urls_clean)} URLs\")\n",
    "\n",
    "for url in shop_urls_clean[:num_items]:\n",
    "    print(f\"Processing {url}\")\n",
    "    res = al_scraper(url=url, show=False)\n",
    "    db_j[url] = res\n",
    "\n",
    "print(\"done!\")\n",
    "json.dump(db_j, db_tmp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MongoDB Santiy Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read and parse the JSON file\n",
    "with open(f\"{DUMP_DIR}/soon-to-be-db.json\", \"r\") as f:\n",
    "    db_data = json.load(f)\n",
    "\n",
    "# Print basic stats\n",
    "print(f\"Number of events scraped: {len(db_data)}\")\n",
    "\n",
    "# Print data for all events\n",
    "for event_url, event_data in db_data.items():\n",
    "    print(\"\\nEvent data:\")\n",
    "    print(f\"URL: {event_url}\")\n",
    "    print(f\"Number of shop items: {len(event_data['shop_items'])}\")\n",
    "    if \"dates\" in event_data:\n",
    "        print(\"Event dates:\")\n",
    "        for server, dates in event_data[\"dates\"].items():\n",
    "            print(f\"  {server}: {dates}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload new data to mongodb collection (name set, ignores env table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully connected to MongoDB!\n",
      "Successfully inserted 63 documents into MongoDB\n"
     ]
    }
   ],
   "source": [
    "load_dotenv(\"../.env\")\n",
    "db_url = os.getenv(\"DB_URL\")\n",
    "client = MongoClient(db_url)\n",
    "\n",
    "# Test connection\n",
    "try:\n",
    "    client.admin.command(\"ping\")\n",
    "    print(\"Successfully connected to MongoDB!\")\n",
    "    with open(f\"{DUMP_DIR}/soon-to-be-db.json\", \"r\") as f:\n",
    "        db_data = json.load(f)\n",
    "\n",
    "    # Get the database and collection\n",
    "    db = client[\"azurlane\"]\n",
    "    events_collection = db[\"events\"]\n",
    "\n",
    "    # Convert the data into a list of documents\n",
    "    documents = []\n",
    "    for url, event_data in db_data.items():\n",
    "        doc = event_data.copy()\n",
    "        doc[\"url\"] = url  # Add URL as a field in the document\n",
    "        documents.append(doc)\n",
    "\n",
    "    # Insert the documents\n",
    "    if documents:\n",
    "        result = events_collection.insert_many(documents)\n",
    "        print(\n",
    "            f\"Successfully inserted {len(result.inserted_ids)} documents into MongoDB\"\n",
    "        )\n",
    "    else:\n",
    "        print(\"No documents to insert\")\n",
    "except Exception as e:\n",
    "    print(f\"Failed to connect to MongoDB: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sanity check read (specify url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Found event data for https://azurlane.koumakan.jp/wiki/Opposite-Colored_Rerun:\n",
      "{\n",
      "    \"_id\": \"674886835a170b03e520d00a\",\n",
      "    \"dates\": {\n",
      "        \"en_server\": {\n",
      "            \"start\": \"October 31st, 2019\",\n",
      "            \"end\": \"November 13th, 2019\"\n",
      "        }\n",
      "    },\n",
      "    \"shop_items\": [\n",
      "        {\n",
      "            \"name\": \"Tirpitz\",\n",
      "            \"stock\": \"1\",\n",
      "            \"price\": \"8000\",\n",
      "            \"image_url\": \"https://azurlane.netojuu.com/images/thumb/5/5e/TirpitzIcon.png/96px-TirpitzIcon.png\"\n",
      "        },\n",
      "        {\n",
      "            \"name\": \"Gneisenau\",\n",
      "            \"stock\": \"1\",\n",
      "            \"price\": \"4000\",\n",
      "            \"image_url\": \"https://azurlane.netojuu.com/images/thumb/4/49/GneisenauIcon.png/96px-GneisenauIcon.png\"\n",
      "        },\n",
      "        {\n",
      "            \"name\": \"533mm Quintuple Homing Torpedo Mount\",\n",
      "            \"stock\": \"1\",\n",
      "            \"price\": \"6000\",\n",
      "            \"image_url\": \"https://azurlane.netojuu.com/images/thumb/7/7b/45200.png/96px-45200.png\"\n",
      "        },\n",
      "        {\n",
      "            \"name\": \"Twin 380mm (SK C/34)\",\n",
      "            \"stock\": \"1\",\n",
      "            \"price\": \"2000\",\n",
      "            \"image_url\": \"https://azurlane.netojuu.com/images/thumb/c/c2/44100.png/96px-44100.png\"\n",
      "        },\n",
      "        {\n",
      "            \"name\": \"533mm Quadruple Homing Torpedo Mount\",\n",
      "            \"stock\": \"1\",\n",
      "            \"price\": \"2000\",\n",
      "            \"image_url\": \"https://azurlane.netojuu.com/images/thumb/5/51/45100.png/96px-45100.png\"\n",
      "        },\n",
      "        {\n",
      "            \"name\": \"Messerschmitt Me-155A\",\n",
      "            \"stock\": \"1\",\n",
      "            \"price\": \"2000\",\n",
      "            \"image_url\": \"https://azurlane.netojuu.com/images/thumb/f/f9/47100.png/96px-47100.png\"\n",
      "        },\n",
      "        {\n",
      "            \"name\": \"Compressed Oxygen Cylinder\",\n",
      "            \"stock\": \"1\",\n",
      "            \"price\": \"1000\",\n",
      "            \"image_url\": \"https://azurlane.netojuu.com/images/thumb/9/9b/3100.png/96px-3100.png\"\n",
      "        },\n",
      "        {\n",
      "            \"name\": \"Swordfish Mark II-ASV (Anti-Sub)\",\n",
      "            \"stock\": \"1\",\n",
      "            \"price\": \"1000\",\n",
      "            \"image_url\": \"https://azurlane.netojuu.com/images/thumb/4/4e/4200.png/96px-4200.png\"\n",
      "        },\n",
      "        {\n",
      "            \"name\": \"Cognitive Chips\",\n",
      "            \"stock\": \"10\",\n",
      "            \"price\": \"300\",\n",
      "            \"image_url\": \"https://azurlane.netojuu.com/images/thumb/6/69/Cognitive_Chip.png/96px-Cognitive_Chip.png\"\n",
      "        },\n",
      "        {\n",
      "            \"name\": \"Strengthening Unit S1\",\n",
      "            \"stock\": \"10\",\n",
      "            \"price\": \"500\",\n",
      "            \"image_url\": \"https://azurlane.netojuu.com/images/thumb/4/49/AllSUnit.png/96px-AllSUnit.png\"\n",
      "        },\n",
      "        {\n",
      "            \"name\": \"Winery Bath\",\n",
      "            \"stock\": \"1\",\n",
      "            \"price\": \"2000\",\n",
      "            \"image_url\": \"https://azurlane.netojuu.com/images/thumb/8/89/FurnIcon_pijiuchigericon.png/96px-FurnIcon_pijiuchigericon.png\"\n",
      "        },\n",
      "        {\n",
      "            \"name\": \"Wine Cask Bath\",\n",
      "            \"stock\": \"1\",\n",
      "            \"price\": \"1200\",\n",
      "            \"image_url\": \"https://azurlane.netojuu.com/images/thumb/8/8a/FurnIcon_yuganggericon.png/96px-FurnIcon_yuganggericon.png\"\n",
      "        },\n",
      "        {\n",
      "            \"name\": \"Wine Cask Bed\",\n",
      "            \"stock\": \"2\",\n",
      "            \"price\": \"300\",\n",
      "            \"image_url\": \"https://azurlane.netojuu.com/images/thumb/0/09/FurnIcon_chuanggericon.png/96px-FurnIcon_chuanggericon.png\"\n",
      "        },\n",
      "        {\n",
      "            \"name\": \"Fancy Bar\",\n",
      "            \"stock\": \"1\",\n",
      "            \"price\": \"300\",\n",
      "            \"image_url\": \"https://azurlane.netojuu.com/images/thumb/8/82/FurnIcon_gerbataiicon.png/96px-FurnIcon_gerbataiicon.png\"\n",
      "        },\n",
      "        {\n",
      "            \"name\": \"Chateau Sofa\",\n",
      "            \"stock\": \"2\",\n",
      "            \"price\": \"300\",\n",
      "            \"image_url\": \"https://azurlane.netojuu.com/images/thumb/7/7c/FurnIcon_shafagericon.png/96px-FurnIcon_shafagericon.png\"\n",
      "        },\n",
      "        {\n",
      "            \"name\": \"Oil\",\n",
      "            \"stock\": \"5\",\n",
      "            \"price\": \"450\",\n",
      "            \"image_url\": \"https://azurlane.netojuu.com/images/thumb/f/f2/Oilicon.png/96px-Oilicon.png\"\n",
      "        },\n",
      "        {\n",
      "            \"name\": \"Naval Curry\",\n",
      "            \"stock\": \"5\",\n",
      "            \"price\": \"50\",\n",
      "            \"image_url\": \"https://azurlane.netojuu.com/images/thumb/b/bc/Food4.png/96px-Food4.png\"\n",
      "        }\n",
      "    ],\n",
      "    \"url\": \"https://azurlane.koumakan.jp/wiki/Opposite-Colored_Rerun\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Query for specific event by URL\n",
    "event_url = \"https://azurlane.koumakan.jp/wiki/Opposite-Colored_Rerun\"\n",
    "event = events_collection.find_one({\"url\": event_url})\n",
    "\n",
    "if event:\n",
    "    print(f\"\\nFound event data for {event_url}:\")\n",
    "    print(json.dumps(event, indent=4, default=str))\n",
    "else:\n",
    "    print(f\"No event found with URL: {event_url}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
